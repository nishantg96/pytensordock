{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])"},"docs":[{"location":"","text":"PyTensordock API Reference class pytensordock.api. TensorDockWrapper ( api_key , api_token , debug=False ) Initialize the TensorDockAPIWrapper. Parameters api_key (str) \u2014 The API key for authentication. api_token (str) \u2014 The API token for authentication. Methods delete_server ( server_uuid ) (dict) \u2014 Create a request to delete a server. deploy_machine ( name , gpu_count , gpu_model , vcpus , ram , external_ports , internal_ports , hostnode , storage , operating_system , password , deployment_type , cpu_model , location , cloudinit_script , price_type , price ) (dict) \u2014 This endpoint allows a single deployment of a machine, based on parameters you control. Pass in, via the REST body, the following variables., as shown with some examples get_specific_hostnode ( id ) (dict) \u2014 Instead of returning all available hostnodes, you can also return the information of a specific one based on its UUID. get_vm_details ( server_uuid ) (dict) \u2014 Retrieve the details of a specific virtual machine. list_authorizations ( ) (dict(str: any)) \u2014 Get a list of all authorizations. list_available_hostnodes ( min_vcpus , min_ram , min_storage , min_vram , min_gpu_count , requires_rtx , requires_gtx ) (dict) \u2014 This gets a list of all available hostnodes and stock for GPUs. These hostnodes are categorized as \"local storage\". \"Network storage\" GPUs can be provisioned by following the instructions listed under the deployment endpoint. You will need to keep track of the GPU model and available CPU/RAM for the deployment step. All params are optional, but can be added to filter for certain hostnodes. list_virtual_machines ( ) (dict) \u2014 List all current virtual machines registered under an organization. modify_server ( server_uuid , gpu_model , gpu_count , ram , vcpus , storage ) (dict) \u2014 Modify the specifications of a virtual machine. Your virtual machine must be of type \"network storage\" and stopped before modifying. retrieve_balance ( ) (dict) \u2014 Through this endpoint, you can easily retrieve your current balance and spending rate to monitor your balance. soft_validate_existing_spot_instance ( server , price ) (dict) \u2014 To validate if an existing VM, modified to this price, will succeed, you can send this request. soft_validate_new_spot_instance ( gpu_count , gpu_model , vcpus , hostnode , ram , storage , price ) (dict) \u2014 To validate if an interruptible instance of a given price will succeed, you can send this request. start_server ( vm_uuid ) (dict) \u2014 Create a request to start a server on an authorization. stop_server ( server_uuid , disassociate_resources ) (dict) \u2014 Create a request to stop a server on an authorization. If the server is stopped without releasing the GPU, it will be billed at the same rate as a running server. If the GPU is released, it will only be billed for the storage costs. test_authorization ( ) (dict) \u2014 Here you can test that an authorization is registered and working. method stop_server ( server_uuid , disassociate_resources=True ) Create a request to stop a server on an authorization. If the server is stopped without releasing the GPU, it will be billed at the same rate as a running server. If the GPU is released, it will only be billed for the storage costs. Parameters server_uuid (str) \u2014 The UUID of the server to stop. disassociate_resources (bool) \u2014 Optional - set to \"True\" if you want to release the GPU when stopping the VM. Returns (dict) The JSON response. method start_server ( vm_uuid ) Create a request to start a server on an authorization. Parameters vm_uuid (str) \u2014 The UUID of the virtual machine to start. Returns (dict) The JSON response. method modify_server ( server_uuid , gpu_model , gpu_count , ram , vcpus , storage ) Modify the specifications of a virtual machine. Your virtual machine must be of type \"network storage\" and stopped before modifying. Parameters server_uuid (str) \u2014 The UUID of the virtual machine to modify. gpu_model (str) \u2014 You can get available GPU models on a hostnode from the hostnodes list API endpoint. Examples: geforcertx3090-pcie-24gb, rtxa6000-pcie-48gb, etc. gpu_count (int) \u2014 Number of GPUs. ram (int) \u2014 RAM amount. vcpus (int) \u2014 Number of vCPUs. storage (int) \u2014 Storage amount. Returns (dict) The JSON response. method delete_server ( server_uuid ) Create a request to delete a server. Parameters server_uuid (str) \u2014 The UUID of the virtual machine to delete. Returns (dict) The JSON response. method list_virtual_machines ( ) List all current virtual machines registered under an organization. Returns (dict) The JSON response. method get_vm_details ( server_uuid ) Retrieve the details of a specific virtual machine. Parameters server_uuid (str) \u2014 The UUID of the virtual machine to retrieve details. Returns (dict) The JSON response. method soft_validate_new_spot_instance ( gpu_count , gpu_model , vcpus , hostnode , ram , storage , price ) To validate if an interruptible instance of a given price will succeed, you can send this request. If the \"success\" field is false, then you must bid higher and confirm that enough resources are available. If the \"success\" field is true, then a deployment of the resources you selected will succeed. Parameters gpu_count (int) \u2014 Number of GPUs. gpu_model (str) \u2014 GPU model. vcpus (int) \u2014 Number of vCPUs. hostnode (str) \u2014 Hostnode ID. ram (int) \u2014 RAM amount. storage (int) \u2014 Storage amount. price (float) \u2014 Bid price for the spot instance. Returns (dict) The JSON response. method soft_validate_existing_spot_instance ( server , price ) To validate if an existing VM, modified to this price, will succeed, you can send this request. If the \"success\" field is false, then you must bid higher and confirm that enough resources are available. If the \"success\" field is true, then your VM will start with the new price (or your VM will continue to run at the new price). Parameters server (str) \u2014 The UUID of the virtual machine to validate. price (float) \u2014 Bid price for the spot instance. Returns (dict) The JSON response. method deploy_machine ( name , gpu_count , gpu_model , vcpus , ram , external_ports , internal_ports , hostnode , storage , operating_system , password , deployment_type='local' , cpu_model=None , location=None , cloudinit_script=None , price_type=None , price=None ) This endpoint allows a single deployment of a machine, based on parameters you control. Pass in, via the REST body, the following variables., as shown with some examples Ports should be sent as a list structed with curly braces with commas and spaces as separators of the ports themselves. The first index of the externally requested port will forward into the first index of the internally requested port, and so on... You can view available ports for each machine through the hostnodes list API. Parameters name (str) \u2014 Name of your VM to be displayed in the dashboard gpu_count (int) \u2014 Number of GPUs. gpu_model (str) \u2014 You can get available GPU models on a hostnode from the hostnodes list API endpoint. Examples: geforcertx3090-pcie-24gb, rtxa6000-pcie-48gb, etc. vcpus (int) \u2014 Number of vCPUs. ram (int) \u2014 RAM amount. external_ports (list) \u2014 External port mappings. internal_ports (list) \u2014 Internal port mappings. hostnode (str) \u2014 Hostnode ID. storage (int) \u2014 Storage amount. operating_system (str) \u2014 Operating system. password (str) \u2014 Password for the virtual machine. deployment_type (str) \u2014 Optional field to specify either a \"network\" or \"local\" deployment. For CPU-only deployments, use \"network\". Defaults to \"local\". cpu_model (str) \u2014 Required if deploying a CPU-only server. location (str) \u2014 Required if deployment type is \"network\". Can be either \"New York City, New York, United States\", \"Chicago, Illinois, United States\", or \"Las Vegas, Nevada, United States\". cloudinit_script (str) \u2014 String of text to append to our cloud-init script. price_type (str) \u2014 Optional field to deploy a spot instance. price (float) \u2014 Optional field to set bid amount for spot deployment. Returns (dict) The JSON response. method list_available_hostnodes ( min_vcpus=None , min_ram=None , min_storage=None , min_vram=None , min_gpu_count=None , requires_rtx=None , requires_gtx=None ) This gets a list of all available hostnodes and stock for GPUs. These hostnodes are categorized as \"local storage\". \"Network storage\" GPUs can be provisioned by following the instructions listed under the deployment endpoint. You will need to keep track of the GPU model and available CPU/RAM for the deployment step. All params are optional, but can be added to filter for certain hostnodes. Note: If you include your organization's API key and token in the request, like such: https://marketplace.tensordock.com/api/v0/client/deploy/hostnodes?api_key=KEY&api_token=TOKEN Hostnodes that are reserved for your organization will also show up in the list. Parameters min_vcpus (int) \u2014 Minimum number of vCPUs. min_ram (int) \u2014 Minimum amount of RAM. min_storage (int) \u2014 Minimum SSD storage amount in GB. min_vram (int) \u2014 Minimum VRAM amount. min_gpu_count (int) \u2014 Minimum number of GPUs. requires_rtx (bool) \u2014 Requires RTX GPU. requires_gtx (bool) \u2014 Requires GTX GPU. Returns (dict) The JSON response. method list_authorizations ( ) \u2192 dict(str: any) Get a list of all authorizations. method retrieve_balance ( ) Through this endpoint, you can easily retrieve your current balance and spending rate to monitor your balance. Remember, once your balance runs out, your servers are automatically deleted \u2014 so please constantly monitor and understand your billing situation to ensure that this does not occur. Returns (dict) The JSON response. method test_authorization ( ) Here you can test that an authorization is registered and working. Simply pass in your API key as api_key and API token as api_token, and confirm that we return true! We will return an object {\"success\": true} if your authorization key and token are valid. Returns (dict) The JSON response. method get_specific_hostnode ( id ) Instead of returning all available hostnodes, you can also return the information of a specific one based on its UUID. Parameters id (str) \u2014 The ID of the hostnode. Returns (dict) The JSON response.","title":"Home"},{"location":"#pytensordock-api-reference","text":"class","title":"PyTensordock API Reference"},{"location":"#pytensordockapitensordockwrapper","text":"Initialize the TensorDockAPIWrapper. Parameters api_key (str) \u2014 The API key for authentication. api_token (str) \u2014 The API token for authentication. Methods delete_server ( server_uuid ) (dict) \u2014 Create a request to delete a server. deploy_machine ( name , gpu_count , gpu_model , vcpus , ram , external_ports , internal_ports , hostnode , storage , operating_system , password , deployment_type , cpu_model , location , cloudinit_script , price_type , price ) (dict) \u2014 This endpoint allows a single deployment of a machine, based on parameters you control. Pass in, via the REST body, the following variables., as shown with some examples get_specific_hostnode ( id ) (dict) \u2014 Instead of returning all available hostnodes, you can also return the information of a specific one based on its UUID. get_vm_details ( server_uuid ) (dict) \u2014 Retrieve the details of a specific virtual machine. list_authorizations ( ) (dict(str: any)) \u2014 Get a list of all authorizations. list_available_hostnodes ( min_vcpus , min_ram , min_storage , min_vram , min_gpu_count , requires_rtx , requires_gtx ) (dict) \u2014 This gets a list of all available hostnodes and stock for GPUs. These hostnodes are categorized as \"local storage\". \"Network storage\" GPUs can be provisioned by following the instructions listed under the deployment endpoint. You will need to keep track of the GPU model and available CPU/RAM for the deployment step. All params are optional, but can be added to filter for certain hostnodes. list_virtual_machines ( ) (dict) \u2014 List all current virtual machines registered under an organization. modify_server ( server_uuid , gpu_model , gpu_count , ram , vcpus , storage ) (dict) \u2014 Modify the specifications of a virtual machine. Your virtual machine must be of type \"network storage\" and stopped before modifying. retrieve_balance ( ) (dict) \u2014 Through this endpoint, you can easily retrieve your current balance and spending rate to monitor your balance. soft_validate_existing_spot_instance ( server , price ) (dict) \u2014 To validate if an existing VM, modified to this price, will succeed, you can send this request. soft_validate_new_spot_instance ( gpu_count , gpu_model , vcpus , hostnode , ram , storage , price ) (dict) \u2014 To validate if an interruptible instance of a given price will succeed, you can send this request. start_server ( vm_uuid ) (dict) \u2014 Create a request to start a server on an authorization. stop_server ( server_uuid , disassociate_resources ) (dict) \u2014 Create a request to stop a server on an authorization. If the server is stopped without releasing the GPU, it will be billed at the same rate as a running server. If the GPU is released, it will only be billed for the storage costs. test_authorization ( ) (dict) \u2014 Here you can test that an authorization is registered and working. method","title":"pytensordock.api.TensorDockWrapper"},{"location":"#pytensordockapitensordockwrapperstop_server","text":"Create a request to stop a server on an authorization. If the server is stopped without releasing the GPU, it will be billed at the same rate as a running server. If the GPU is released, it will only be billed for the storage costs. Parameters server_uuid (str) \u2014 The UUID of the server to stop. disassociate_resources (bool) \u2014 Optional - set to \"True\" if you want to release the GPU when stopping the VM. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.stop_server"},{"location":"#pytensordockapitensordockwrapperstart_server","text":"Create a request to start a server on an authorization. Parameters vm_uuid (str) \u2014 The UUID of the virtual machine to start. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.start_server"},{"location":"#pytensordockapitensordockwrappermodify_server","text":"Modify the specifications of a virtual machine. Your virtual machine must be of type \"network storage\" and stopped before modifying. Parameters server_uuid (str) \u2014 The UUID of the virtual machine to modify. gpu_model (str) \u2014 You can get available GPU models on a hostnode from the hostnodes list API endpoint. Examples: geforcertx3090-pcie-24gb, rtxa6000-pcie-48gb, etc. gpu_count (int) \u2014 Number of GPUs. ram (int) \u2014 RAM amount. vcpus (int) \u2014 Number of vCPUs. storage (int) \u2014 Storage amount. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.modify_server"},{"location":"#pytensordockapitensordockwrapperdelete_server","text":"Create a request to delete a server. Parameters server_uuid (str) \u2014 The UUID of the virtual machine to delete. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.delete_server"},{"location":"#pytensordockapitensordockwrapperlist_virtual_machines","text":"List all current virtual machines registered under an organization. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.list_virtual_machines"},{"location":"#pytensordockapitensordockwrapperget_vm_details","text":"Retrieve the details of a specific virtual machine. Parameters server_uuid (str) \u2014 The UUID of the virtual machine to retrieve details. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.get_vm_details"},{"location":"#pytensordockapitensordockwrappersoft_validate_new_spot_instance","text":"To validate if an interruptible instance of a given price will succeed, you can send this request. If the \"success\" field is false, then you must bid higher and confirm that enough resources are available. If the \"success\" field is true, then a deployment of the resources you selected will succeed. Parameters gpu_count (int) \u2014 Number of GPUs. gpu_model (str) \u2014 GPU model. vcpus (int) \u2014 Number of vCPUs. hostnode (str) \u2014 Hostnode ID. ram (int) \u2014 RAM amount. storage (int) \u2014 Storage amount. price (float) \u2014 Bid price for the spot instance. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.soft_validate_new_spot_instance"},{"location":"#pytensordockapitensordockwrappersoft_validate_existing_spot_instance","text":"To validate if an existing VM, modified to this price, will succeed, you can send this request. If the \"success\" field is false, then you must bid higher and confirm that enough resources are available. If the \"success\" field is true, then your VM will start with the new price (or your VM will continue to run at the new price). Parameters server (str) \u2014 The UUID of the virtual machine to validate. price (float) \u2014 Bid price for the spot instance. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.soft_validate_existing_spot_instance"},{"location":"#pytensordockapitensordockwrapperdeploy_machine","text":"This endpoint allows a single deployment of a machine, based on parameters you control. Pass in, via the REST body, the following variables., as shown with some examples Ports should be sent as a list structed with curly braces with commas and spaces as separators of the ports themselves. The first index of the externally requested port will forward into the first index of the internally requested port, and so on... You can view available ports for each machine through the hostnodes list API. Parameters name (str) \u2014 Name of your VM to be displayed in the dashboard gpu_count (int) \u2014 Number of GPUs. gpu_model (str) \u2014 You can get available GPU models on a hostnode from the hostnodes list API endpoint. Examples: geforcertx3090-pcie-24gb, rtxa6000-pcie-48gb, etc. vcpus (int) \u2014 Number of vCPUs. ram (int) \u2014 RAM amount. external_ports (list) \u2014 External port mappings. internal_ports (list) \u2014 Internal port mappings. hostnode (str) \u2014 Hostnode ID. storage (int) \u2014 Storage amount. operating_system (str) \u2014 Operating system. password (str) \u2014 Password for the virtual machine. deployment_type (str) \u2014 Optional field to specify either a \"network\" or \"local\" deployment. For CPU-only deployments, use \"network\". Defaults to \"local\". cpu_model (str) \u2014 Required if deploying a CPU-only server. location (str) \u2014 Required if deployment type is \"network\". Can be either \"New York City, New York, United States\", \"Chicago, Illinois, United States\", or \"Las Vegas, Nevada, United States\". cloudinit_script (str) \u2014 String of text to append to our cloud-init script. price_type (str) \u2014 Optional field to deploy a spot instance. price (float) \u2014 Optional field to set bid amount for spot deployment. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.deploy_machine"},{"location":"#pytensordockapitensordockwrapperlist_available_hostnodes","text":"This gets a list of all available hostnodes and stock for GPUs. These hostnodes are categorized as \"local storage\". \"Network storage\" GPUs can be provisioned by following the instructions listed under the deployment endpoint. You will need to keep track of the GPU model and available CPU/RAM for the deployment step. All params are optional, but can be added to filter for certain hostnodes. Note: If you include your organization's API key and token in the request, like such: https://marketplace.tensordock.com/api/v0/client/deploy/hostnodes?api_key=KEY&api_token=TOKEN Hostnodes that are reserved for your organization will also show up in the list. Parameters min_vcpus (int) \u2014 Minimum number of vCPUs. min_ram (int) \u2014 Minimum amount of RAM. min_storage (int) \u2014 Minimum SSD storage amount in GB. min_vram (int) \u2014 Minimum VRAM amount. min_gpu_count (int) \u2014 Minimum number of GPUs. requires_rtx (bool) \u2014 Requires RTX GPU. requires_gtx (bool) \u2014 Requires GTX GPU. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.list_available_hostnodes"},{"location":"#pytensordockapitensordockwrapperlist_authorizations","text":"Get a list of all authorizations. method","title":"pytensordock.api.TensorDockWrapper.list_authorizations"},{"location":"#pytensordockapitensordockwrapperretrieve_balance","text":"Through this endpoint, you can easily retrieve your current balance and spending rate to monitor your balance. Remember, once your balance runs out, your servers are automatically deleted \u2014 so please constantly monitor and understand your billing situation to ensure that this does not occur. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.retrieve_balance"},{"location":"#pytensordockapitensordockwrappertest_authorization","text":"Here you can test that an authorization is registered and working. Simply pass in your API key as api_key and API token as api_token, and confirm that we return true! We will return an object {\"success\": true} if your authorization key and token are valid. Returns (dict) The JSON response. method","title":"pytensordock.api.TensorDockWrapper.test_authorization"},{"location":"#pytensordockapitensordockwrapperget_specific_hostnode","text":"Instead of returning all available hostnodes, you can also return the information of a specific one based on its UUID. Parameters id (str) \u2014 The ID of the hostnode. Returns (dict) The JSON response.","title":"pytensordock.api.TensorDockWrapper.get_specific_hostnode"},{"location":"api/","text":"","title":"Api"}]}