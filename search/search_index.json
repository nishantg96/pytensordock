{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyTensordock API Reference","text":""},{"location":"#pytensordock.api.TensorDockWrapper","title":"<code>pytensordock.api.TensorDockWrapper</code>","text":""},{"location":"#pytensordock.api.TensorDockWrapper.__init__","title":"<code>__init__(api_key, api_token, debug=False)</code>","text":"<p>Initialize the TensorDockAPIWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for authentication.</p> required <code>api_token</code> <code>str</code> <p>The API token for authentication.</p> required"},{"location":"#pytensordock.api.TensorDockWrapper.delete_server","title":"<code>delete_server(server_uuid)</code>","text":"<p>Create a request to delete a server.</p> <p>Parameters:</p> Name Type Description Default <code>server_uuid</code> <code>str</code> <p>The UUID of the virtual machine to delete.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.deploy_machine","title":"<code>deploy_machine(name, gpu_count, gpu_model, vcpus, ram, external_ports, internal_ports, hostnode, storage, operating_system, password, deployment_type='local', cpu_model=None, location=None, cloudinit_script=None, price_type=None, price=None)</code>","text":"<pre><code>    This endpoint allows a single deployment of a machine, based on parameters you control. Pass in, via the REST body, the following variables., as shown with some examples\n\n    Ports should be sent as a list structed with curly braces with commas and spaces as separators of the ports themselves. The first index of the externally requested port will forward into the first index of the internally requested port, and so on... You can view available ports for each machine through the hostnodes list API.\n\n    Args:\n        name (str): Name of your VM to be displayed in the dashboard\n        gpu_count (int): Number of GPUs.\n        gpu_model (str): You can get available GPU models on a hostnode from the hostnodes list API endpoint. Examples: geforcertx3090-pcie-24gb, rtxa6000-pcie-48gb, etc.\n        vcpus (int): Number of vCPUs.\n        ram (int): RAM amount.\n        external_ports (list): External port mappings.\n        internal_ports (list): Internal port mappings.\n        hostnode (str): Hostnode ID.\n        storage (int): Storage amount.\n        operating_system (str): Operating system.\n        password (str): Password for the virtual machine.\n        deployment_type (str): Optional field to specify either a \"network\" or \"local\" deployment. For CPU-only deployments, use \"network\". Defaults to \"local\".\n        cpu_model (str): Required if deploying a CPU-only server.\n        location (str): Required if deployment type is \"network\". Can be either \"New York City, New York, United States\", \"Chicago, Illinois, United States\", or \"Las Vegas, Nevada, United States\".\n        cloudinit_script (str): String of text to append to our cloud-init script, with newlines substituted for\n</code></pre> <p>.             price_type (str): Optional field to deploy a spot instance.             price (float): Optional field to set bid amount for spot deployment.</p> <pre><code>    Returns:\n        dict: The JSON response.\n</code></pre>"},{"location":"#pytensordock.api.TensorDockWrapper.get_specific_hostnode","title":"<code>get_specific_hostnode(id)</code>","text":"<p>Instead of returning all available hostnodes, you can also return the information of a specific one based on its UUID.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The ID of the hostnode.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.get_vm_details","title":"<code>get_vm_details(server_uuid)</code>","text":"<p>Retrieve the details of a specific virtual machine.</p> <p>Parameters:</p> Name Type Description Default <code>server_uuid</code> <code>str</code> <p>The UUID of the virtual machine to retrieve details.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.list_authorizations","title":"<code>list_authorizations()</code>","text":"<p>Get a list of all authorizations.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.list_available_hostnodes","title":"<code>list_available_hostnodes(min_vcpus=None, min_ram=None, min_storage=None, min_vram=None, min_gpu_count=None, requires_rtx=None, requires_gtx=None)</code>","text":"<p>This gets a list of all available hostnodes and stock for GPUs. These hostnodes are categorized as \"local storage\". \"Network storage\" GPUs can be provisioned by following the instructions listed under the deployment endpoint. You will need to keep track of the GPU model and available CPU/RAM for the deployment step. All params are optional, but can be added to filter for certain hostnodes.</p> <p>Note: If you include your organization's API key and token in the request, like such:</p> <p>https://marketplace.tensordock.com/api/v0/client/deploy/hostnodes?api_key=KEY&amp;api_token=TOKEN</p> <p>Hostnodes that are reserved for your organization will also show up in the list.</p> <p>Parameters:</p> Name Type Description Default <code>min_vcpus</code> <code>int</code> <p>Minimum number of vCPUs.</p> <code>None</code> <code>min_ram</code> <code>int</code> <p>Minimum amount of RAM.</p> <code>None</code> <code>min_storage</code> <code>int</code> <p>Minimum SSD storage amount in GB.</p> <code>None</code> <code>min_vram</code> <code>int</code> <p>Minimum VRAM amount.</p> <code>None</code> <code>min_gpu_count</code> <code>int</code> <p>Minimum number of GPUs.</p> <code>None</code> <code>requires_rtx</code> <code>bool</code> <p>Requires RTX GPU.</p> <code>None</code> <code>requires_gtx</code> <code>bool</code> <p>Requires GTX GPU.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.list_virtual_machines","title":"<code>list_virtual_machines()</code>","text":"<p>List all current virtual machines registered under an organization.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.modify_server","title":"<code>modify_server(server_uuid, gpu_model, gpu_count, ram, vcpus, storage)</code>","text":"<p>Modify the specifications of a virtual machine. Your virtual machine must be of type \"network storage\" and stopped before modifying.</p> <p>Parameters:</p> Name Type Description Default <code>server_uuid</code> <code>str</code> <p>The UUID of the virtual machine to modify.</p> required <code>gpu_model</code> <code>str</code> <p>You can get available GPU models on a hostnode from the hostnodes list API endpoint. Examples: geforcertx3090-pcie-24gb, rtxa6000-pcie-48gb, etc.</p> required <code>gpu_count</code> <code>int</code> <p>Number of GPUs.</p> required <code>ram</code> <code>int</code> <p>RAM amount.</p> required <code>vcpus</code> <code>int</code> <p>Number of vCPUs.</p> required <code>storage</code> <code>int</code> <p>Storage amount.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.retrieve_balance","title":"<code>retrieve_balance()</code>","text":"<p>Through this endpoint, you can easily retrieve your current balance and spending rate to monitor your balance.</p> <p>Remember, once your balance runs out, your servers are automatically deleted \u2014 so please constantly monitor and understand your billing situation to ensure that this does not occur.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.soft_validate_existing_spot_instance","title":"<code>soft_validate_existing_spot_instance(server, price)</code>","text":"<p>To validate if an existing VM, modified to this price, will succeed, you can send this request.</p> <p>If the \"success\" field is false, then you must bid higher and confirm that enough resources are available. If the \"success\" field is true, then your VM will start with the new price (or your VM will continue to run at the new price).</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str</code> <p>The UUID of the virtual machine to validate.</p> required <code>price</code> <code>float</code> <p>Bid price for the spot instance.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.soft_validate_new_spot_instance","title":"<code>soft_validate_new_spot_instance(gpu_count, gpu_model, vcpus, hostnode, ram, storage, price)</code>","text":"<p>To validate if an interruptible instance of a given price will succeed, you can send this request.</p> <p>If the \"success\" field is false, then you must bid higher and confirm that enough resources are available. If the \"success\" field is true, then a deployment of the resources you selected will succeed.</p> <p>Parameters:</p> Name Type Description Default <code>gpu_count</code> <code>int</code> <p>Number of GPUs.</p> required <code>gpu_model</code> <code>str</code> <p>GPU model.</p> required <code>vcpus</code> <code>int</code> <p>Number of vCPUs.</p> required <code>hostnode</code> <code>str</code> <p>Hostnode ID.</p> required <code>ram</code> <code>int</code> <p>RAM amount.</p> required <code>storage</code> <code>int</code> <p>Storage amount.</p> required <code>price</code> <code>float</code> <p>Bid price for the spot instance.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.start_server","title":"<code>start_server(vm_uuid)</code>","text":"<p>Create a request to start a server on an authorization.</p> <p>Parameters:</p> Name Type Description Default <code>vm_uuid</code> <code>str</code> <p>The UUID of the virtual machine to start.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.stop_server","title":"<code>stop_server(server_uuid, disassociate_resources=True)</code>","text":"<p>Create a request to stop a server on an authorization. If the server is stopped without releasing the GPU, it will be billed at the same rate as a running server. If the GPU is released, it will only be billed for the storage costs.</p> <p>Parameters:</p> Name Type Description Default <code>server_uuid</code> <code>str</code> <p>The UUID of the server to stop.</p> required <code>disassociate_resources</code> <code>bool</code> <p>Optional - set to \"True\" if you want to release the GPU when stopping the VM.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"},{"location":"#pytensordock.api.TensorDockWrapper.test_authorization","title":"<code>test_authorization()</code>","text":"<p>Here you can test that an authorization is registered and working.</p> <p>Simply pass in your API key as api_key and API token as api_token, and confirm that we return true!</p> <p>We will return an object {\"success\": true} if your authorization key and token are valid.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The JSON response.</p>"}]}